
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Model averaging &#8212; PyMC3 3.3 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '3.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayes Factors and Marginal Likelihood" href="Bayes_factor.html" />
    <link rel="prev" title="Model comparison" href="model_comparison.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }
</style>
<div class="section" id="Model-averaging">
<h1>Model averaging<a class="headerlink" href="#Model-averaging" title="Permalink to this headline">¶</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-darkgrid&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Runing on PyMC3 v{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Runing on PyMC3 v3.3
</pre></div></div>
</div>
<p>When confronted with more than one model we have several options. One of
them is to perform model selection, using for example a given
Information Criterion as exemplified <a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/model_comparison.html">in this
notebook</a>
and this other
<a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/GLM-model-selection.html">example</a>.
Model selection is appealing for its simplicity, but we are discarding
information about the uncertainty in our models. This is somehow similar
to computing the full posterior and then just keep a point-estimate like
the posterior mean; we may become overconfident of what we really know.</p>
<p>One alternative is to perform model selection but discuss all the
different models together with the computed values of a given
Information Criterion. It is important to put all these numbers and
tests in the context of our problem in a way that ourselves and our
audience can have a better feeling of the possible limitations and
shortcomings of our methods. If you are in the academic world you can
use this approach to add elements to the discussion section of a paper,
presentation, thesis, and so on.</p>
<p>Yet another approach is to perform model averaging. The idea is to
generate a meta-model (and meta-predictions) using a weighted average of
the models. There are several ways to do this and PyMC3 includes 3 of
them. In this notebook we are going to briefly discuss them, you will
find a more thorough explanation in the work by <a class="reference external" href="https://arxiv.org/abs/1704.02030">Yuling Yao et.
al.</a></p>
<div class="section" id="Pseudo-Bayesian-model-averaging">
<h2>Pseudo Bayesian model averaging<a class="headerlink" href="#Pseudo-Bayesian-model-averaging" title="Permalink to this headline">¶</a></h2>
<p>Bayesian models can be weighted by their marginal likelihood, this is
known as Bayesian Model Averaging. While this is theoretically
appealing, is problematic in practice: on the one hand the marginal
likelihood is highly sensitive to the specification of the prior, in a
way that parameter estimation is not, and on the other hand computing
the marginal likelihood is usually a challenging task. An alternative
route is to use the values of WAIC (Widely Applicable Information
Criterion) or LOO (pareto-smoothed importance sampling Leave-One-Out
cross-validation), which we will call generically IC, to estimate
weights. We can do this by using the following formula:</p>
<div class="math">
\[w_i = \frac {e^{ \frac{1}{2} dIC_i }} {\sum_j^M e^{ - \frac{1}{2} dIC_j }}\]</div>
<p>Where <span class="math">\(dIC_i\)</span> is the difference between the i-esim information
criterion value and the lowest one. Remember that the lowest the value
of the IC, the better. We can use any information criterion we want to
compute a set of weights, but, of course, we cannot mix them.</p>
<p>This approach is called pseudo Bayesian model averaging, or Akaike-like
weighting and is an heuristic way to compute the relative probability of
each model (given a fixed set of models) from the information criteria
values. Look how the denominator is just a normalization term to ensure
that the weights sum up to one.</p>
</div>
<div class="section" id="Pseudo-Bayesian-model-averaging-with-Bayesian-Bootstrapping">
<h2>Pseudo Bayesian model averaging with Bayesian Bootstrapping<a class="headerlink" href="#Pseudo-Bayesian-model-averaging-with-Bayesian-Bootstrapping" title="Permalink to this headline">¶</a></h2>
<p>The above formula for computing weights is a very nice and simple
approach, but with one major caveat it does not take into account the
uncertainty in the computation of the IC. We could compute the standard
error of the IC (assuming a Gaussian approximation) and modify the above
formula accordingly. Or we can do something more robust, like using a
<a class="reference external" href="http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/">Bayesian
Bootstrapping</a>
to estimate, and incorporate this uncertainty.</p>
</div>
<div class="section" id="Stacking">
<h2>Stacking<a class="headerlink" href="#Stacking" title="Permalink to this headline">¶</a></h2>
<p>The third approach implemented in PyMC3 is know as <em>stacking of
predictive distributions</em> and it has been recently
<a class="reference external" href="https://arxiv.org/abs/1704.02030">proposed</a>. We want to combine
several models in a metamodel in order to minimize the diverge between
the meta-model and the <em>true</em> generating model, when using a logarithmic
scoring rule this is equivalently to:</p>
<div class="math">
\[\max_{n} \frac{1}{n} \sum_{i=1}^{n}log\sum_{k=1}^{K} w_k p(y_i|y_{-i}, M_k)\]</div>
<p>Where <span class="math">\(n\)</span> is the number of data points and <span class="math">\(K\)</span> the number of
models. To enforce a solution we constrain <span class="math">\(w\)</span> to be
<span class="math">\(w_k \ge 0\)</span> and <span class="math">\(\sum_{k=1}^{K} w_k = 1\)</span>.</p>
<p>The quantity <span class="math">\(p(y_i|y_{-i}, M_k)\)</span> is the leave-one-out predictive
distribution for the <span class="math">\(M_k\)</span> model. Computing it requires fitting
each model <span class="math">\(n\)</span> times, each time leaving out one data point.
Fortunately we can approximate the exact leave-one-out predictive
distribution using LOO (or even WAIC), and that is what we do in
practice.</p>
</div>
<div class="section" id="Weighted-posterior-predictive-samples">
<h2>Weighted posterior predictive samples<a class="headerlink" href="#Weighted-posterior-predictive-samples" title="Permalink to this headline">¶</a></h2>
<p>Once we have computed the weights, using any of the above 3 methods, we
can use them to get a weighted posterior predictive samples. PyMC3
offers functions to perform these steps in a simple way, so let see them
in action using an example.</p>
<p>The following example is taken from the superb book <a class="reference external" href="http://xcelab.net/rm/statistical-rethinking/">Statistical
Rethinking</a> by Richard
McElreath. You will find more PyMC3 examples from this book in this
<a class="reference external" href="https://github.com/pymc-devs/resources/tree/master/Rethinking">repository</a>.
We are going to explore a simplified version of it. Check the book for
the whole example and a more thorough discussion of both, the biological
motivation for this problem and a theoretical/practical discussion of
using Information Criteria to compare, select and average models.</p>
<p>Briefly, our problem is as follows: We want to explore the composition
of milk across several primate species, it is hypothesized that females
from species of primates with larger brains produce more <em>nutritious</em>
milk (loosely speaking this is done <em>in order to</em> support the
development of such big brains). This is an important question for
evolutionary biologists and we will try to provide an answer using 3
variables.</p>
<ul class="simple">
<li>Two predictor variables:<ul>
<li>The proportion of neocortex compared to the total mass of the
brain</li>
<li>The logarithm of the body mass of the mothers</li>
</ul>
</li>
<li>One predicted variable, the kilocalories per gram of milk.</li>
</ul>
<p>With these variables we are going to build 3 different linear models:</p>
<ol class="arabic simple">
<li>A model using only the neocortex variable</li>
<li>A model using only the logarithm of the mass variable</li>
<li>A model using both variables</li>
</ol>
<p>Let start by uploading the data and centering the <code class="docutils literal"><span class="pre">neocortex</span></code> and
<code class="docutils literal"><span class="pre">log</span> <span class="pre">mass</span></code> variables, for better sampling.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">d</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/milk.csv&#39;</span><span class="p">)</span>
<span class="n">d</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">d</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">d</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>kcal.per.g</th>
      <th>neocortex</th>
      <th>log_mass</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.49</td>
      <td>-0.123706</td>
      <td>-0.831353</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.47</td>
      <td>-0.030706</td>
      <td>0.158647</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.56</td>
      <td>-0.030706</td>
      <td>0.181647</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.89</td>
      <td>0.000294</td>
      <td>-0.579353</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.92</td>
      <td>0.012294</td>
      <td>-1.885353</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Now that we have the data we are going to build our first model using
only the <code class="docutils literal"><span class="pre">neocortex</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_0</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;neocortex&#39;</span><span class="p">]</span>

    <span class="n">kcal</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;kcal&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;kcal.per.g&#39;</span><span class="p">])</span>
    <span class="n">trace_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [sigma_log__, beta, alpha]
100%|██████████| 2500/2500 [00:02&lt;00:00, 1073.98it/s]
</pre></div></div>
</div>
<p>The second model is exactly the same as the first one, except we now use
the logarithm of the mass</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_1</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;log_mass&#39;</span><span class="p">]</span>

    <span class="n">kcal</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;kcal&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;kcal.per.g&#39;</span><span class="p">])</span>

    <span class="n">trace_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [sigma_log__, beta, alpha]
100%|██████████| 2500/2500 [00:01&lt;00:00, 1469.12it/s]
The acceptance probability does not match the target. It is 0.885079201825, but should be close to 0.8. Try to increase the number of tuning steps.
</pre></div></div>
</div>
<p>And finally the third model using the <code class="docutils literal"><span class="pre">neocortex</span></code> and <code class="docutils literal"><span class="pre">log_mass</span></code>
variables</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_2</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">d</span><span class="p">[[</span><span class="s1">&#39;neocortex&#39;</span><span class="p">,</span><span class="s1">&#39;log_mass&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="n">kcal</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;kcal&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;kcal.per.g&#39;</span><span class="p">])</span>

    <span class="n">trace_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [sigma_log__, beta, alpha]
100%|██████████| 2500/2500 [00:02&lt;00:00, 985.94it/s]
</pre></div></div>
</div>
<p>Now that we have sampled the posterior for the 3 models, we are going to
compare them visually. One option is to use the <code class="docutils literal"><span class="pre">forestplot</span></code> function
that supports plotting more than one trace.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">traces</span> <span class="o">=</span> <span class="p">[</span><span class="n">trace_0</span><span class="p">,</span> <span class="n">trace_1</span><span class="p">,</span> <span class="n">trace_2</span><span class="p">]</span>
<span class="n">pm</span><span class="o">.</span><span class="n">forestplot</span><span class="p">(</span><span class="n">traces</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_model_averaging_11_0.png" src="../_images/notebooks_model_averaging_11_0.png" />
</div>
</div>
<p>Another option is to plot several traces in a same plot, we can achieve
this with the <code class="docutils literal"><span class="pre">densityplot</span></code> function. This plot is somehow similar to
a forestplot, but we get truncated KDE plots (by default 95% credible
intervals) grouped by variable names together with a point estimate (by
default the mean).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">pm</span><span class="o">.</span><span class="n">densityplot</span><span class="p">(</span><span class="n">traces</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_model_averaging_13_0.png" src="../_images/notebooks_model_averaging_13_0.png" />
</div>
</div>
<p>Now that we have sampled the posterior for the 3 models, we are going to
use WAIC (Widely applicable information criterion) to compare the 3
models. We can do this using the <code class="docutils literal"><span class="pre">compare</span></code> function included with
PyMC3.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">,</span> <span class="n">model_2</span><span class="p">]</span>
<span class="n">comp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BB-pseudo-BMA&#39;</span><span class="p">)</span>
<span class="n">comp</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
/home/osvaldo/proyectos/00_PyMC3/pymc3/pymc3/stats.py:194: UserWarning: For one or more samples the posterior variance of the
        log predictive densities exceeds 0.4. This could be indication of
        WAIC starting to fail see http://arxiv.org/abs/1507.04544 for details

  &#34;&#34;&#34;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WAIC</th>
      <th>pWAIC</th>
      <th>dWAIC</th>
      <th>weight</th>
      <th>SE</th>
      <th>dSE</th>
      <th>var_warn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>-15.61</td>
      <td>2.55</td>
      <td>0</td>
      <td>0.89</td>
      <td>4.82</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-8.97</td>
      <td>1.97</td>
      <td>6.64</td>
      <td>0.04</td>
      <td>3.85</td>
      <td>2.17</td>
      <td>0</td>
    </tr>
    <tr>
      <th>0</th>
      <td>-7.45</td>
      <td>1.85</td>
      <td>8.16</td>
      <td>0.07</td>
      <td>3.02</td>
      <td>4.11</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>We can see that the best model is <code class="docutils literal"><span class="pre">model_2</span></code>, the one with both
predictor variables. Notice the DataFrame is ordered from lowest to
highest WAIC (<em>i.e</em> from <em>better</em> to <em>worst</em> model). Check <a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/model_comparison.html">this
notebook</a>
for a more detailed discussing on model comparison.</p>
<p>We can also see that we get a column with the relative <code class="docutils literal"><span class="pre">weight</span></code> for
each model (according to the first equation at the beginning of this
notebook). This weights can be <em>vaguely</em> interpreted as the probability
that each model will make the correct predictions on future data. Of
course this interpretation is conditional on the models used to compute
the weights, if we add or remove models the weights will change. And
also is dependent on the assumptions behind WAIC (or any other
Information Criterion used). So try to do not overinterpret these
<code class="docutils literal"><span class="pre">weights</span></code>.</p>
<p>Now we are going to use the computed <code class="docutils literal"><span class="pre">weights</span></code> to generate predictions
based not on a single model but on the weighted set of models. Using
PyMC3 we can call the <code class="docutils literal"><span class="pre">sample_ppc_w</span></code> function as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">ppc_w</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_ppc_w</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span>
                        <span class="n">weights</span><span class="o">=</span><span class="n">comp</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">sort_index</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                        <span class="n">progressbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Notice that we are passing the weights ordered by their index. We are
doing this because we pass <code class="docutils literal"><span class="pre">traces</span></code> and <code class="docutils literal"><span class="pre">models</span></code> ordered from model
0 to 2, but the computed weights are ordered from lowest to highest WAIC
(or equivalently from larger to lowest weight). In summary, we must be
sure that we are correctly pairing the weights and models.</p>
<p>We are also going to compute PPCs for the lowest-WAIC model</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">ppc_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_ppc</span><span class="p">(</span><span class="n">trace_2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">model_2</span><span class="p">,</span>
                      <span class="n">progressbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>A simple way to compare both kind of predictions is to plot their mean
and hpd interval</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">mean_w</span> <span class="o">=</span> <span class="n">ppc_w</span><span class="p">[</span><span class="s1">&#39;kcal&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">hpd_w</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">hpd</span><span class="p">(</span><span class="n">ppc_w</span><span class="p">[</span><span class="s1">&#39;kcal&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">ppc_2</span><span class="p">[</span><span class="s1">&#39;kcal&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">hpd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">hpd</span><span class="p">(</span><span class="n">ppc_2</span><span class="p">[</span><span class="s1">&#39;kcal&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xerr</span><span class="o">=</span><span class="p">[[</span><span class="n">mean</span> <span class="o">-</span> <span class="n">hpd</span><span class="p">]],</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;model 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">mean_w</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xerr</span><span class="o">=</span><span class="p">[[</span><span class="n">mean_w</span> <span class="o">-</span> <span class="n">hpd_w</span><span class="p">]],</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;weighted models&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;kcal per g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_model_averaging_21_0.png" src="../_images/notebooks_model_averaging_21_0.png" />
</div>
</div>
<p>As we can see the mean value is almost the same for both predictions but
the uncertainty in the weighted model is larger. We have effectively
propagated the uncertainty about which model we should select to the
posterior predictive samples. You can now try with the other two methods
for computing weights <code class="docutils literal"><span class="pre">stacking</span></code> (the default and recommended method)
and <code class="docutils literal"><span class="pre">pseudo-BMA</span></code>.</p>
<p><strong>Final notes:</strong></p>
<p>There are other ways to average models such as, for example, explicitly
building a meta-model that includes all the models we have. We then
perform parameter inference while jumping between the models. One
problem with this approach is that jumping between models could hamper
the proper sampling of the posterior.</p>
<p>Besides averaging discrete models we can sometimes think of continuous
versions of them. A toy example is to imagine that we have a coin and we
want to estimated it’s degree of bias, a number between 0 and 1 being
0.5 equal chance of head and tails. We could think of two separated
models one with a prior biased towards heads and one towards tails. We
could fit both separate models and then average them using, for example,
IC-derived weights. An alternative, is to build a hierarchical model to
estimate the prior distribution, instead of contemplating two discrete
models we will be computing a continuous model that includes these the
discrete ones as particular cases. Which approach is better? That
depends on our concrete problem. Do we have good reasons to think about
two discrete models, or is our problem better represented with a
continuous bigger model?</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/pymc3_logo.jpg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">PyMC3</a></h1>



<p class="blurb">Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano</p>






<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prob_dists.html">Probability Distributions</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../examples.html#howto">Howto</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="sampler-stats.html">Sampler statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="Diagnosing_biased_Inference_with_Divergences.html">Diagnosing Biased Inference with Divergences</a></li>
<li class="toctree-l3"><a class="reference internal" href="posterior_predictive.html">Posterior Predictive Checks</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_comparison.html">Model comparison</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Model averaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="Bayes_factor.html">Bayes Factors and Marginal Likelihood</a></li>
<li class="toctree-l3"><a class="reference internal" href="howto_debugging.html">How to debug a model</a></li>
<li class="toctree-l3"><a class="reference internal" href="PyMC3_tips_and_heuristic.html">PyMC3 Modeling tips and heuristic</a></li>
<li class="toctree-l3"><a class="reference internal" href="LKJ.html">LKJ Cholesky Covariance Priors for Multivariate Normal Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="live_sample_plots.html">Live sample plots</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#applied">Applied</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#glm">GLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#gaussian-processes">Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#mixture-models">Mixture Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#variational-inference">Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#stochastic-gradient">Stochastic Gradient</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, The PyMC Development Team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/notebooks/model_averaging.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>